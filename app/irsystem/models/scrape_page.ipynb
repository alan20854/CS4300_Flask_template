{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from bs4.element import Comment\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://en.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the links in the paragraphs for wikipedia URL\n",
    "def get_links_for_url(url):\n",
    "    url_title = url[url.rfind('wiki')+5:]\n",
    "    print(\"reading page: \" + url_title)\n",
    "    page = requests.get(url)\n",
    "    try:\n",
    "        soup = BeautifulSoup(page.text, \"html5lib\")\n",
    "        tags = soup.find_all('p')\n",
    "        res_links = []\n",
    "        for tag in tags:\n",
    "            links = tag.find_all('a', attrs={'href': re.compile(\"^/wiki/\")})\n",
    "            for link in links:\n",
    "                res_links.append(link.get('href'))\n",
    "        return res_links\n",
    "    except Exception as e:\n",
    "        print(\"Error loading page\")\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get cleaned text in the paragraph for wikipedia URL\n",
    "def get_text_for_url(url):\n",
    "    url_title = url[url.rfind('wiki')+5:]\n",
    "    print(\"reading page: \" + url_title)\n",
    "    page = requests.get(url)\n",
    "    try:\n",
    "        soup = BeautifulSoup(page.text, \"html5lib\")\n",
    "        text = soup.find_all('p')\n",
    "        full_text = \"\"\n",
    "        num = 0\n",
    "        for t in text:\n",
    "            if num > 5:\n",
    "                break\n",
    "            if not t.find('img') and (t.name == 'p' or t.name == 'ul'):\n",
    "                text = str(t.getText().replace('\\n', ''))\n",
    "                full_text += re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text) + \" \"\n",
    "                num += 1\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(\"Error loading page\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links_for_url(base_url + '/wiki/Computer_science')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text_for_url(base_url + '/wiki/Computer_science')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(u, v):\n",
    "    return (u[0].union(v[0]), u[1] + v[1])\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    text = regex.sub(' ', text)\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def clean_text_for_vectorizer(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    text = regex.sub(' ', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_representation_for_course_description(course_description, branching_factor, search_depth, representation):\n",
    "    '''\n",
    "    course_description: [str], string of text description for a course\n",
    "    branching_factor: float, in range(0, 1), what percentage of the links we want to traverse at each step\n",
    "    search_depth: the maximum depth of link traversals starting from an original word in the course_description\n",
    "    \n",
    "    Returns: (links, representation)\n",
    "    '''\n",
    "    course_description\n",
    "    res = (set(), \"\")\n",
    "    explored_urls = set()\n",
    "    for word in course_description:\n",
    "        url = '/wiki/' + word\n",
    "        res = reduce(res, generate_representation_for_word(explored_urls, url, branching_factor, search_depth, \"\"))\n",
    "    return res\n",
    "    \n",
    "    \n",
    "\n",
    "def generate_representation_for_word(explored_urls, url, branching_factor, search_depth, representation):\n",
    "    '''\n",
    "    explored_urls: set, of urls that have already been explored\n",
    "    url: the url we wish to find the wikipedia article on, \n",
    "    if word is multiple words, it will appear as /wiki/computer_science, /wiki/computer_graphics, etc\n",
    "    branching_factor: float, in range(0, 1), which percentage of the links we want to traverse at each step\n",
    "    search_depth: int, the maximum depth of link traversals starting from word\n",
    "    \n",
    "    return: (links, represenation)\n",
    "    links: list of strings\n",
    "    representation: string\n",
    "    '''\n",
    "    if search_depth == 0 or url in explored_urls:\n",
    "        return (set(), \"\")\n",
    "    explored_urls.add(url)\n",
    "    links = get_links_for_url(base_url + url)\n",
    "    if links is None: # Only if the url is not valid\n",
    "        return (explored_urls, representation)\n",
    "    text = get_text_for_url(base_url + url)\n",
    "    representation += \" \" + text\n",
    "    next_links = links[:min(8, int(branching_factor * len(links)))]\n",
    "    \n",
    "    res = (explored_urls, representation)\n",
    "    for link in next_links:\n",
    "        res = reduce(res, generate_representation_for_word(explored_urls, link, branching_factor, search_depth - 1, \"\"))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_for_cs_course = generate_representation_for_course_description([\"Computer_science\", \"biology\"], 0.05, 2, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_for_cs_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rep_for_Computer_science = generate_representation_for_word(set(), '/wiki/Computer_science', 0.05, 2, \"\")\n",
    "# rep_for_Computer_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/courseroster/full_json.txt\") as f:\n",
    "    cornell_course_descriptions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_course_descriptions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_course_descriptions['CS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "course_codes = []\n",
    "all_major_courses = [cornell_course_descriptions[key] for key in cornell_course_descriptions.keys()]\n",
    "print(all_major_courses[0:100])\n",
    "all_courses = []\n",
    "for major_courses in all_major_courses:\n",
    "    all_courses += major_courses\n",
    "\n",
    "for i, course_data in enumerate(all_courses):\n",
    "    course_number = course_data['courseNumber']\n",
    "    course_title = course_data['courseTitle']\n",
    "    course_desc = course_data['description']\n",
    "    cleaned_course_desc = clean_text_for_vectorizer(course_desc)\n",
    "    if cleaned_course_desc != None and cleaned_course_desc != []: \n",
    "        corpus.append(cleaned_course_desc)\n",
    "        course_codes.append(course_data['subject'] + ' ' + course_number)\n",
    "    course_outcome = None\n",
    "    try:\n",
    "        course_outcome = course_data['outcome']\n",
    "    except Exception as e:\n",
    "        course_outcome = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[4000])\n",
    "print(len(course_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ctomm\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase= True, stop_words='english', max_df=0.7, min_df = 2, smooth_idf=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "doc_by_vocab = vectorizer.fit_transform([d for d in corpus]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_tfidf_terms_from_text(vectorizer, text, n):\n",
    "    print(\"S\")\n",
    "    if text is None:\n",
    "        return []\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    transformed_text = vectorizer.transform([text])\n",
    "    tfidf_sorting = np.argsort(transformed_text.toarray()).flatten()[::-1]\n",
    "    k = len(text.split())\n",
    "    top_n = feature_array[tfidf_sorting][:min(k, n)]\n",
    "    return top_n\n",
    "\n",
    "def get_top_n_important_terms_from_text(vectorizer, text, n):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs4110 = \"Introduction to the design of systems programs, with emphasis on multiprogrammed operating systems. Topics include concurrency, synchronization, deadlocks, memory management, protection, input-output methods, networking, file systems and security. The impact of network and distributed computing environments on operating systems is also discussed.\"\n",
    "cs2300 = \"Web programming requires the cooperation of two machines: the one in front of the viewer (client) and the one delivering the content (server). INFO 1300 concentrates almost exclusively on the client side. The main emphasis in INFO 2300 is learning about server side processing. Students begin with a short overview of the PHP server-side scripting language, then look at interactions with databases, learning about querying via the database language SQL. Through a succession of projects, students learn how to apply this understanding to the creation of an interactive, data-driven site via PHP and the MYSQL database. Also considered are technologies such as Javascript and Ajax and techniques to enhance security and privacy. Design and usability issues are emphasized. A major component of the course is the creation of a substantial web site.\"\n",
    "cs2110 = \"Intermediate programming in a high-level language and introduction to computer science. Topics include object-oriented programming (classes, objects, subclasses, types), graphical user interfaces, algorithm analysis (asymptotic complexity, big O notation), recursion, testing, program correctness (loop invariants), searching/sorting, data structures (lists, trees, stacks, queues, heaps, search trees, hash tables, graphs), graph algorithms. Java is the principal programming language.\"\n",
    "psych1101 = \"This course provides an introduction to the science of the mind.  Everyone knows what it's like to think and perceive, but this subjective experience provides little insight into how minds emerge from physical intities like brains.  To address this issue, cognitive science integrates work from at least five disciplines: Psychology, Neuroscience, Computer Science, Linguistics, and Philosophy.  This course introduces students to the insights these disciplines offer into the workings of the mind by exploring visual perception, attention, memory, learning, problem solving, language, and consciousness.\"\n",
    "math2940 = \"linear algebra and its applications  topics include matrices  determinants  vector spaces  eigenvalues and eigenvectors  orthogonality and inner product spaces  applications include brief introductions to difference equations  markov chains  and systems of linear ordinary differential equations  may include computer use in solving problems\"\n",
    "\n",
    "print(get_top_n_tfidf_terms_from_text(vectorizer, clean_text_for_vectorizer(cs2300), 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test_x = vectorizer.transform([cs2300])\n",
    "print(test_x.shape)\n",
    "\n",
    "res = cosine_similarity(X, test_x).flatten()\n",
    "top_res = np.argsort(res)[::-1]\n",
    "print(top_res[0:15])    \n",
    "for res in top_res[0:15]:\n",
    "    print(corpus[res])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "# pickle.dump(X, open(\"tdm.pkl\", \"wb\"))\n",
    "#pickle.dump(corpus, open(\"corpus.pkl\", \"wb\"))\n",
    "pickle.dump(course_codes, open(\"course_codes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_codes = list(cornell_course_descriptions.keys())\n",
    "print(major_codes)\n",
    "all_courses = []\n",
    "for major_code in major_codes:\n",
    "    for course_data in cornell_course_descriptions[major_code]:\n",
    "        all_courses.append(course_data)\n",
    "print(len(all_courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_representations = {}\n",
    "for course_data in all_courses:\n",
    "    course_number = course_data['courseNumber']\n",
    "    dept = course_data['subject']\n",
    "    course_desc = course_data['description']\n",
    "    cleaned_course_desc = clean_text_for_vectorizer(course_desc)\n",
    "    print(course_number)\n",
    "    print(course_desc)\n",
    "    print(cleaned_course_desc)\n",
    "    top_tfidf_words = get_top_n_tfidf_terms_from_text(vectorizer, cleaned_course_desc, 20)\n",
    "    print(top_tfidf_words)\n",
    "    course_representation = ' '.join(top_tfidf_words)\n",
    "    course_representations[dept + ' ' +str(course_number)] = course_representation\n",
    "    print(course_representation)\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_representations['CS 3410']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(course_representations, open(\"all_courses_20_tfidf_representations.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_json = {'tags': []}\n",
    "added_tags = []\n",
    "for major in cornell_course_descriptions.keys():\n",
    "    major_desc = \"\"\n",
    "    for course_data in cornell_course_descriptions[major]:\n",
    "        if course_data['description'] != None:\n",
    "            major_desc += \" \" + clean_text_for_vectorizer(course_data['description'])\n",
    "    top_tfidf_words = get_top_n_tfidf_terms_from_text(vectorizer, major_desc, 50)\n",
    "#     print(top_tfidf_words)\n",
    "    for word in top_tfidf_words:\n",
    "        if word not in added_tags:\n",
    "            tag_json['tags'].append({'tag': word})\n",
    "            added_tags.append(word)\n",
    "print (tag_json['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(tag_json, open( \"tag_json.txt\", \"w\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
